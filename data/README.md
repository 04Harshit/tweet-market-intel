# Data Directory Structure

This directory contains all data generated by the market intelligence system.

## Structure
data/
├── raw/ # Raw collected tweets
│ ├── 20240101/ # Partitioned by date
│ └── 20240102/
├── processed/ # Cleaned and processed data
│ ├── 20240101/
│ └── 20240102/
├── signals/ # Generated trading signals
│ └── signals_20240101_120000.parquet
├── models/ # Trained models
│ └── model_20240101_120000.joblib
├── visualizations/ # Generated visualizations
│ ├── dashboard_20240101_120000.html
│ ├── streaming_20240101_120000.html
│ └── heatmap_20240101_120000.html
└── analysis/ # Analysis results
└── results_20240101_120000.json

text

## Data Formats

### Raw Tweets
- Format: Parquet (partitioned by date/hour)
- Columns: tweet_id, username, content, timestamp, engagement metrics, etc.
- Size: ~1-2KB per tweet

### Processed Data
- Format: Parquet
- Columns: Cleaned content, extracted features, normalized metrics
- Includes: content_hash for deduplication

### Signals
- Format: Parquet
- Columns: Timestamp, composite_signal, confidence intervals, volume metrics
- Aggregated: By time windows (default: 60 minutes)

## Sample Data

Sample data files are provided in each directory to demonstrate the format:

1. `raw/sample_tweets.parquet` - 100 sample tweets
2. `processed/sample_cleaned.parquet` - Cleaned sample data
3. `signals/sample_signals.parquet` - Generated signals from sample data
4. `visualizations/sample_dashboard.html` - Sample visualization

## Usage

### Loading Data
```python
import pandas as pd

# Load raw tweets
df_raw = pd.read_parquet('data/raw/sample_tweets.parquet')

# Load signals
df_signals = pd.read_parquet('data/signals/sample_signals.parquet')
Querying Partitioned Data

python
from src.storage.parquet_handler import ParquetHandler

handler = ParquetHandler(config)
df = handler.query_data(
    'data/raw',
    start_date='2024-01-01',
    end_date='2024-01-02',
    min_engagement=10
)
Data Retention

By default:

Raw data: 30 days
Processed data: 90 days
Signals: 180 days
Models: Keep latest 10 versions
Cleanup can be run manually:

bash
python scripts/cleanup_data.py --retention-days 30
Notes

All data is compressed using Snappy compression
Partitioning enables efficient time-based queries
Unicode and Indian language characters are properly handled
Data includes confidence intervals for signals
